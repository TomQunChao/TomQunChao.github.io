<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>nlp学习3-BERT系列学习 | Hexo</title>

  <!-- keywords -->
  

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="RNN循环神经网络 编码器和解码器编码器即把自然语言转化为向量(一般为二维)，解码器即把特征向量转化为人类可阅读的语言 Attention注意力机制到底是什么——基于常识的基本结构介绍 - 李鹏宇的文章 - 知乎 用于自动学习中计算数据对输出数据的贡献大小，即有选择性地处理信号 特征工程就是选择和目标最有关系的特征，减少工作量。一般的特征工程为各个特征赋予固定权值，而注意力机制让模型自己学习如何分">
<meta property="og:type" content="article">
<meta property="og:title" content="nlp学习3-BERT系列学习">
<meta property="og:url" content="http://example.com/2021/02/27/note/papers/ai/nlp-3/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="RNN循环神经网络 编码器和解码器编码器即把自然语言转化为向量(一般为二维)，解码器即把特征向量转化为人类可阅读的语言 Attention注意力机制到底是什么——基于常识的基本结构介绍 - 李鹏宇的文章 - 知乎 用于自动学习中计算数据对输出数据的贡献大小，即有选择性地处理信号 特征工程就是选择和目标最有关系的特征，减少工作量。一般的特征工程为各个特征赋予固定权值，而注意力机制让模型自己学习如何分">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/.com//attention-1.jpg">
<meta property="og:image" content="http://example.com/.com//transformer-1.jpg">
<meta property="og:image" content="http://example.com/.com//self-attention-1.jpg">
<meta property="og:image" content="http://example.com/.com//self-attention-2.jpg">
<meta property="article:published_time" content="2021-02-27T13:01:47.000Z">
<meta property="article:modified_time" content="2021-03-01T08:23:54.690Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/.com//attention-1.jpg">
  
    <link rel="alternative" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="http://7xkj1z.com1.z0.glb.clouddn.com/head.jpg">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  

  
<script src="//cdn.bootcss.com/require.js/2.3.2/require.min.js"></script>

  
<script src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script>


  
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.2.0"></head>
<body>
  <div id="container">
    <div id="particles-js"></div>
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="https://raw.githubusercontent.com/TomQunChao/avatar/main/icon00.jpg" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/"></a></h1>
		</hgroup>

		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						<div class="icon-wrap icon-link hide" data-idx="2">
							<div class="loopback_l"></div>
							<div class="loopback_r"></div>
						</div>
						
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>菜单</li>
						<li>标签</li>
						
						<li>友情链接</li>
						
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">主页</a></li>
				        
							<li><a href="/archives">所有文章</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						
					</div>
				</section>
				
				
				
				<section class="switch-part switch-part3">
					<div id="js-friends">
					
			          <a target="_blank" class="main-nav-link switch-friends-link" href="https://github.com/smackgg/hexo-theme-smackdown">smackdown</a>
			        
			        </div>
				</section>
				

				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide"></h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img lazy-src="https://raw.githubusercontent.com/TomQunChao/avatar/main/icon00.jpg" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author"></h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/archives">所有文章</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
				</div>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap"><article id="post-note/papers/ai/nlp-3" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2021/02/27/note/papers/ai/nlp-3/" class="article-date">
  	<time datetime="2021-02-27T13:01:47.000Z" itemprop="datePublished">2021-02-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      nlp学习3-BERT系列学习
      
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
        

        
        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><p>循环神经网络</p>
<h2 id="编码器和解码器"><a href="#编码器和解码器" class="headerlink" title="编码器和解码器"></a>编码器和解码器</h2><p>编码器即把自然语言转化为向量(一般为二维)，解码器即把特征向量转化为人类可阅读的语言</p>
<h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/105335191">注意力机制到底是什么——基于常识的基本结构介绍 - 李鹏宇的文章 - 知乎</a></p>
<p>用于自动学习中计算数据对输出数据的贡献大小，即有选择性地处理信号</p>
<p>特征工程就是选择和目标最有关系的特征，减少工作量。一般的特征工程为各个特征赋予固定权值，而注意力机制让模型自己学习如何分配注意力(特征加权)。</p>
<p>注意力机制位于编码器和解码器之间，用于计算各个输出向量对解码器每一个输出的影响程度</p>
<p><img src="/.com//attention-1.jpg"><br>$W_1 = (w_{1,1},w_{1,2},w_{1,3},w_{1,4})$</p>
<p>$w_{i,j}=g(y_i,z_j)$</p>
<p>$C_1=\begin{bmatrix}W_1\W_1\W_1\W_1\end{bmatrix}*\begin{bmatrix}z1\z2\z3\z4\end{bmatrix}$</p>
<p>$y_1=f_{de}(c1,y_0)$</p>
<p>直到解码器输出了&lt;end&gt;标签。</p>
<p>让解码器根据自身历史输出计算$y_i$与$Z$的相关性，即可得到$W$矩阵</p>
<p>可以看出，注意力机制增加了额外的运算量，降低了推理速度。</p>
<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><p>BERT大火却不懂Transformer？读这一篇就够了 - 数据汪的文章 - 知乎<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/54356280">https://zhuanlan.zhihu.com/p/54356280</a></p>
<p>用放大镜看Transformer——总体和各个模块的结构到底是什么样的 - 李鹏宇的文章 - 知乎<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/122977440">https://zhuanlan.zhihu.com/p/122977440</a></p>
<p>Transformer结构示意图</p>
<p><img src="/.com//transformer-1.jpg"></p>
<p>编码-解码器结构，其中编码器和解码器都利用了类似RNN的时间循环结构。由编码组件、解码组件和它们之间的连接构成。</p>
<p>Transformer会利用上一时刻的输出和当前时刻的输入</p>
<p>编码组件由一堆编码器构成，解码组件由相同数量的解码器构成。</p>
<p>编码器结构上相同，之间未共享参数。每个编码器都可以分解成两个子层：前馈神经网络层和自注意力层。</p>
<p>输入是向量列表，列表大小是可以设置的超参数。</p>
<p>自注意力曾中，各个词有互相连接，互相影响；前馈神经网络曾无相互连接和相互影响，可以并行进行。每一个单词的前馈神经网络完全相同。</p>
<p>自注意力机制会将所有相关单词的理解融入到我们正在处理的单词中。</p>
<h3 id="计算自注意力"><a href="#计算自注意力" class="headerlink" title="计算自注意力"></a>计算自注意力</h3><ol>
<li>使用输入词嵌入后的向量$X$承三个矩阵生成 查询向量、键向量和值向量$Q,K,V$</li>
<li>被打分查询向量$K_s$与每一个键向量$Q_i$点积即得到对应分数</li>
<li>将分数除以维数的平方根，之后使用softmax归一化</li>
<li>将每个值向量$V_i$乘softmax分数</li>
<li>对加权值的向量求和</li>
</ol>
<p>可以用矩阵运算以加快运算速度</p>
<p>这样得到的向量就可以传给前馈网络了</p>
<p>注意力头：使用几个不同的查询/键/值矩阵，计算不同的结果，然后使用一定的方法合并输入到前馈层</p>
<p><img src="/.com//self-attention-1.jpg"></p>
<h3 id="使用位置编码表示序列的顺序"><a href="#使用位置编码表示序列的顺序" class="headerlink" title="使用位置编码表示序列的顺序"></a>使用位置编码表示序列的顺序</h3><p>使用正弦函数进行位置编码</p>
<h3 id="残差连接"><a href="#残差连接" class="headerlink" title="残差连接"></a>残差连接</h3><p>每个编码器中每个子层之间都有一个残差连接，并且都跟随着一个层归一化步骤。相应地，解码器也有这些结构。</p>
<p><img src="/.com//self-attention-2.jpg"></p>
<h3 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h3><blockquote>
<p>编码器通过处理输入序列开启工作。顶端编码器的输出之后会变转化为一个包含向量K（键向量）和V（值向量）的注意力向量集 。这些向量将被每个解码器用于自身的“编码-解码注意力层”，而这些层可以帮助解码器关注输入序列哪些位置合适</p>
</blockquote>
<p>解码器最终输出一个实数，然后会进行线性变换，</p>
<h2 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h2><p>GPT——生成式预训练Transformer - 李鹏宇的文章 - 知乎<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/125139937">https://zhuanlan.zhihu.com/p/125139937</a></p>
<h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><p>基于Transformer的双向编码器表示(BERT)——结构和训练 - 李鹏宇的文章 - 知乎<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/125655365">https://zhuanlan.zhihu.com/p/125655365</a></p>

      
    </div>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2021/02/27/note/web/pentist-4/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">&lt;</strong>
      <div class="article-nav-title">
        
          看靶机渗透测试-4
        
      </div>
    </a>
  
  
    <a href="/2021/02/26/note/linux/usual-1/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">linux常用命令</div>
      <strong class="article-nav-caption">&gt;</strong>
    </a>
  
</nav>

  
</article>


<div class="ds-share share" data-thread-key="note/papers/ai/nlp-3" data-title="nlp学习3-BERT系列学习" data-url="http://example.com/2021/02/27/note/papers/ai/nlp-3/"  data-images="https://raw.githubusercontent.com/TomQunChao/avatar/main/icon00.jpg" data-content="nlp学习3-BERT系列学习">
    <div class="ds-share-inline">
      <ul  class="ds-share-icons-16">
      	<li data-toggle="ds-share-icons-more"><a class="ds-more" href="javascript:void(0);">分享到：</a></li>
        <li><a class="ds-weibo" href="javascript:void(0);" data-service="weibo">微博</a></li>
        <li><a class="ds-qzone" href="javascript:void(0);" data-service="qzone">QQ空间</a></li>
        <li><a class="ds-qqt" href="javascript:void(0);" data-service="qqt">腾讯微博</a></li>
        <li><a class="ds-wechat" href="javascript:void(0);" data-service="wechat">微信</a></li>
      </ul>
      <div class="ds-share-icons-more">
      </div>
    </div>
 </div>
 





</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
      <div class="footer-left">
        &copy; 2021 John Doe
      </div>
        <div class="footer-right">
          <a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/smackgg/hexo-theme-smackdown" target="_blank">Smackdown</a>
        </div>
    </div>
  </div>
</footer>
    </div>
    
  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">



<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>

<script src="/js/main.js"></script>




<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js"></script>


  </div>
</body>
</html>